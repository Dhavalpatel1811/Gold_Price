{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9934ea3e",
   "metadata": {},
   "source": [
    "Best before gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5468db",
   "metadata": {},
   "source": [
    "# rag_engine.py\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"\n",
    "EMB_DIR = \"data/embeddings\"\n",
    "os.makedirs(EMB_DIR, exist_ok=True)\n",
    "\n",
    "def chunk_text(text, size=400):  # Reduced from 500 to 400 for faster processing\n",
    "    \"\"\"Split text into chunks of approximately 'size' words\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), size):\n",
    "        chunk = \" \".join(words[i:i + size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def build_or_load_index(file_path, emb_model):\n",
    "    \"\"\"Build FAISS index or load if already exists\"\"\"\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    index_file = os.path.join(EMB_DIR, f\"{base_name}_index.faiss\")\n",
    "    chunks_file = os.path.join(EMB_DIR, f\"{base_name}_chunks.pkl\")\n",
    "\n",
    "    # Load existing index if available (MUCH FASTER)\n",
    "    if os.path.exists(index_file) and os.path.exists(chunks_file):\n",
    "        print(f\"‚ö° Loading cached index for {os.path.basename(file_path)}...\")\n",
    "        index = faiss.read_index(index_file)\n",
    "        with open(chunks_file, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        return index, chunks\n",
    "\n",
    "    # Build new index (only first time)\n",
    "    print(f\"üî® Building new index for {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: {file_path} not found!\")\n",
    "        return None, []\n",
    "\n",
    "    chunks = chunk_text(text, size=400)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(f\"‚ö†Ô∏è WARNING: No chunks created from {file_path}\")\n",
    "        return None, []\n",
    "\n",
    "    # Create embeddings (no progress bar for speed)\n",
    "    embeddings = emb_model.encode(chunks, show_progress_bar=False)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "\n",
    "    # Save index and chunks\n",
    "    faiss.write_index(index, index_file)\n",
    "    with open(chunks_file, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    print(f\"‚úÖ Index created: {len(chunks)} chunks\")\n",
    "    return index, chunks\n",
    "\n",
    "def setup_rag():\n",
    "    \"\"\"Initialize RAG system with book and prediction indexes\"\"\"\n",
    "    print(\"ü§ñ Loading embedding model...\")\n",
    "    model_emb = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "    print(\"üìö Setting up Book Index...\")\n",
    "    book_index, book_chunks = build_or_load_index(\"data/book.txt\", model_emb)\n",
    "    \n",
    "    print(\"üìä Setting up Prediction Index...\")\n",
    "    pred_index, pred_chunks = build_or_load_index(\"data/predictions/prediction_latest.txt\", model_emb)\n",
    "\n",
    "    return model_emb, book_index, book_chunks, pred_index, pred_chunks\n",
    "\n",
    "def retrieve_context(query, model_emb, book_index=None, book_chunks=None, \n",
    "                     pred_index=None, pred_chunks=None, k=2):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from specified indexes - OPTIMIZED FOR SPEED\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    # Encode the query\n",
    "    query_emb = model_emb.encode([query], show_progress_bar=False).astype('float32')\n",
    "\n",
    "    # Retrieve from book if provided\n",
    "    if book_index is not None and book_chunks:\n",
    "        try:\n",
    "            distances, indices = book_index.search(query_emb, k)\n",
    "            retrieved_chunks = [book_chunks[i] for i in indices[0] if i < len(book_chunks)]\n",
    "            if retrieved_chunks:\n",
    "                # Simpler formatting for speed\n",
    "                context_parts.append(\"Book Context:\\n\" + \"\\n\".join(retrieved_chunks))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error retrieving from book: {e}\")\n",
    "\n",
    "    # Retrieve from predictions if provided\n",
    "    if pred_index is not None and pred_chunks:\n",
    "        try:\n",
    "            distances, indices = pred_index.search(query_emb, k)\n",
    "            retrieved_chunks = [pred_chunks[i] for i in indices[0] if i < len(pred_chunks)]\n",
    "            if retrieved_chunks:\n",
    "                context_parts.append(\"Prediction Data:\\n\" + \"\\n\".join(retrieved_chunks))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error retrieving from predictions: {e}\")\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts) if context_parts else \"No context found.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # main.py\n",
    "from ollama import chat\n",
    "from rag_engine import setup_rag, retrieve_context\n",
    "\n",
    "print(\"üìÑ Loading RAG indexes (book + prediction)...\")\n",
    "model_emb, book_index, book_chunks, pred_index, pred_chunks = setup_rag()\n",
    "print(\"‚úÖ RAG system ready!\\n\")\n",
    "\n",
    "# --- Intent Detection Keywords ---\n",
    "SIMPLE_CHAT = [\"hi\", \"hii\", \"hello\", \"hey\", \"hyy\", \"ok\", \"okay\", \"thanks\", \"thank you\", \n",
    "               \"bye\", \"goodbye\", \"how are you\", \"what's up\", \"wassup\"]\n",
    "\n",
    "INVESTMENT_ADVICE = [\"should i invest\", \"what do you suggest\", \"recommendation\", \n",
    "                     \"what should i do\", \"invest or not\", \"buy or sell\", \"advice\"]\n",
    "\n",
    "PREDICTION_QUERY = [\"prediction\", \"forecast\", \"price tomorrow\", \"future price\", \n",
    "                    \"what will be\", \"trend\", \"next\"]\n",
    "\n",
    "def detect_intent(user_input):\n",
    "    \"\"\"Determine what the user is asking for\"\"\"\n",
    "    q = user_input.lower().strip()\n",
    "    \n",
    "    # Check for simple greetings/chat\n",
    "    if any(word in q for word in SIMPLE_CHAT) and len(q.split()) <= 5:\n",
    "        return \"chat\"\n",
    "    \n",
    "    # Check for investment advice\n",
    "    if any(phrase in q for phrase in INVESTMENT_ADVICE):\n",
    "        return \"investment\"\n",
    "    \n",
    "    # Check for predictions\n",
    "    if any(word in q for word in PREDICTION_QUERY):\n",
    "        return \"prediction\"\n",
    "    \n",
    "    # Default to book QA (theory questions)\n",
    "    return \"book_qa\"\n",
    "\n",
    "\n",
    "def get_response(user_input):\n",
    "    \"\"\"Main function to route user queries - FASTER VERSION\"\"\"\n",
    "    intent = detect_intent(user_input)\n",
    "    \n",
    "    # --- Simple Chat (No RAG needed - FASTEST) ---\n",
    "    if intent == \"chat\":\n",
    "        prompt = f\"Respond briefly and warmly to: {user_input}\"\n",
    "        response = chat(model=\"llama3:latest\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    # --- Investment Advice (Uses both book + prediction) ---\n",
    "    elif intent == \"investment\":\n",
    "        # Retrieve context from BOTH sources (reduced k for speed)\n",
    "        context = retrieve_context(\n",
    "            user_input, \n",
    "            model_emb, \n",
    "            book_index, \n",
    "            book_chunks, \n",
    "            pred_index, \n",
    "            pred_chunks,\n",
    "            k=2  # Reduced from 3 to 2 for faster retrieval\n",
    "        )\n",
    "        \n",
    "        # SHORTER, FASTER PROMPT\n",
    "        prompt = f\"\"\"You're a financial advisor. Give a SHORT recommendation based on:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "\n",
    "Answer format:\n",
    "Recommendation: [INVEST/AVOID/HOLD]\n",
    "Reason: [2-3 sentences combining prediction trend + book principle]\n",
    "\n",
    "Be concise and direct.\"\"\"\n",
    "        \n",
    "        response = chat(model=\"llama3:latest\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    # --- Prediction Query (Only prediction context) ---\n",
    "    elif intent == \"prediction\":\n",
    "        context = retrieve_context(\n",
    "            user_input, \n",
    "            model_emb, \n",
    "            pred_index=pred_index, \n",
    "            pred_chunks=pred_chunks,\n",
    "            k=1  # Only get top result\n",
    "        )\n",
    "        \n",
    "        # SHORTER PROMPT\n",
    "        prompt = f\"\"\"Answer briefly using this prediction data:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\"\"\"\n",
    "        \n",
    "        response = chat(model=\"llama3:latest\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    # --- Book QA (Theory questions) ---\n",
    "    elif intent == \"book_qa\":\n",
    "        context = retrieve_context(\n",
    "            user_input, \n",
    "            model_emb, \n",
    "            book_index=book_index, \n",
    "            book_chunks=book_chunks,\n",
    "            k=2  # Reduced for speed\n",
    "        )\n",
    "        \n",
    "        # SHORTER PROMPT\n",
    "        prompt = f\"\"\"Answer concisely using this book context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\"\"\"\n",
    "        \n",
    "        response = chat(model=\"llama3:latest\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ui.py\n",
    "import gradio as gr\n",
    "import sys\n",
    "from main import get_response\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    \"\"\"Handle incoming messages and maintain chat history\"\"\"\n",
    "    if not message.strip():\n",
    "        return history, \"\"\n",
    "    \n",
    "    # Get response from the bot (now faster!)\n",
    "    response = get_response(message)\n",
    "    \n",
    "    # Append to history\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    return history, \"\"\n",
    "\n",
    "def exit_app():\n",
    "    \"\"\"Gracefully close the application\"\"\"\n",
    "    print(\"\\nüëã Shutting down chatbot... Goodbye!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# --- Custom CSS for Clean UI ---\n",
    "custom_css = \"\"\"\n",
    "#chatbot {\n",
    "    border-radius: 12px;\n",
    "    border: 1px solid #e0e0e0;\n",
    "}\n",
    "\n",
    "#chatbot .message.user {\n",
    "    background-color: #007bff !important;\n",
    "    color: white !important;\n",
    "    border-radius: 18px !important;\n",
    "    padding: 10px 15px !important;\n",
    "    margin: 5px 0 !important;\n",
    "}\n",
    "\n",
    "#chatbot .message.bot {\n",
    "    background-color: #007bff !important;\n",
    "    color: #202124 !important;\n",
    "    border-radius: 18px !important;\n",
    "    padding: 10px 15px !important;\n",
    "    margin: 5px 0 !important;\n",
    "}\n",
    "\n",
    "#input_box {\n",
    "    border-radius: 24px;\n",
    "    border: 1px solid #dadce0;\n",
    "    padding: 10px 20px;\n",
    "}\n",
    "\n",
    ".button-row {\n",
    "    margin-top: 10px;\n",
    "}\n",
    "\n",
    "footer {\n",
    "    display: none !important;\n",
    "}\n",
    "\n",
    "#exit_btn {\n",
    "    background-color: #dc3545 !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# --- Build Gradio Interface ---\n",
    "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # üíº Investment Advisor Chatbot\n",
    "        ### Powered by RAG + Ollama LLaMA3 | The Intelligent Investor + Live Predictions\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"üí¨ Chat\",\n",
    "        type=\"messages\",\n",
    "        height=500,\n",
    "        elem_id=\"chatbot\",\n",
    "        show_copy_button=True\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg_input = gr.Textbox(\n",
    "            placeholder=\"Ask me anything about investing, predictions, or financial theory...\",\n",
    "            show_label=False,\n",
    "            scale=9,\n",
    "            elem_id=\"input_box\"\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "    \n",
    "    with gr.Row(elem_classes=\"button-row\"):\n",
    "        clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\", size=\"sm\")\n",
    "        exit_btn = gr.Button(\"‚ùå Exit\", variant=\"stop\", size=\"sm\", elem_id=\"exit_btn\")\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Tips:**\n",
    "        - Ask theory questions: *\"What is value investing?\"*\n",
    "        - Get predictions: *\"What's the gold price forecast?\"*\n",
    "        - Investment advice: *\"Should I invest in gold?\"*\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    send_btn.click(\n",
    "        fn=chat_interface,\n",
    "        inputs=[msg_input, chatbot],\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        fn=chat_interface,\n",
    "        inputs=[msg_input, chatbot],\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], None),\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    exit_btn.click(\n",
    "        fn=exit_app,\n",
    "        inputs=None,\n",
    "        outputs=None\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Launching Investment Advisor Chatbot...\")\n",
    "    print(\"üìç Open in browser: http://localhost:7860\")\n",
    "    print(\"‚ö° Optimized for faster responses!\")\n",
    "    demo.launch(share=False, server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gold_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
